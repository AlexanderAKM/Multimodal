{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline Project\n",
    "\n",
    "1. Select a Multimodal model (CLIP)?\n",
    "    - We can always extend on this if needed with more modalities\n",
    "2. Define the domains we want to identify.\n",
    "    - Easy example would be if we can identify \"dog\" or \"cat\" units.\n",
    "    - However, our main hope is to identify e.g. \"face processing\" units (similar to FFA) or \"language\" units.\n",
    "3. Define datasets.\n",
    "    - We would need text-only, image-only, and text-image datasets for different procedures.\n",
    "    - Text-only for e.g. words vs. non-words experiment\n",
    "    - Image-only for e.g. face vs. non-face experiment\n",
    "    - Image-only with language involved. \n",
    "    - Text-image to see if specialized modules unify or remain separated (hold off on this but might be interesting).\n",
    "4. Record internal activations at various modules or \"units\".\n",
    "5. Zero out those units vs. random units (similair to paper).\n",
    "6. Conclude that there are e.g. \"face units\" in the model when ablating those units compared to random units impairs face recognition but not other tasks. \n",
    "    - This would then be evidence that multimodal models might specialize similarly to the brain (e.g. occipital lobe in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some code outline (gotten from GPT by giving extensive overview of project and asking for skeleton of code). \n",
    "\n",
    "We should definitely take inspiration from the code of the original paper as well: https://github.com/BKHMSI/llm-localization/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup & Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import clip  # pip install git+https://github.com/openai/CLIP.git\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# DEVICE\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# LOAD A PRETRAINED CLIP MODEL\n",
    "# Possible model names: \"ViT-B/32\", \"ViT-B/16\", \"RN50\", etc.\n",
    "model_name = \"ViT-B/32\"\n",
    "model, preprocess = clip.load(model_name, device=device)\n",
    "model.eval()\n",
    "\n",
    "# model: a CLIP model that has two encoders: model.encode_image(...) and model.encode_text(...)\n",
    "# preprocess: a standard transform for images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Dataset preparation\n",
    "    - We would need text-only and image-only also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptImageTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expects a directory with images, a .csv or .json that maps:\n",
    "      image_path -> text_caption(s), concept_label\n",
    "    This is just a simplified example.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_root, metadata, transform=None):\n",
    "        \"\"\"\n",
    "        data_root: path to images\n",
    "        metadata: list of (img_filename, text, concept_label) or a CSV loaded\n",
    "        transform: image transformations (e.g. 'preprocess' from CLIP)\n",
    "        \"\"\"\n",
    "        self.data_root = data_root\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_fn, text, concept_label = self.metadata[idx]\n",
    "        img_path = os.path.join(self.data_root, img_fn)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, text, concept_label\n",
    "\n",
    "# Example usage (you'd load 'metadata' from a CSV or something similar)\n",
    "# For instance, metadata[i] = (\"dog1.jpg\", \"a dog running\", \"dog\")\n",
    "metadata = [\n",
    "    (\"dog1.jpg\", \"a dog running\", \"dog\"),\n",
    "    (\"cat1.jpg\", \"a cat sleeping\", \"cat\"),\n",
    "    # ...\n",
    "]\n",
    "data_root = \"path/to/image/directory\"\n",
    "\n",
    "dataset = ConceptImageTextDataset(data_root, metadata, transform=preprocess)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Forward Hooks to Extract activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick a layer from the vision encoder to demonstrate hooking\n",
    "# For a ViT, let's say block 0 (lowest-level layer), MLP\n",
    "# Example: hooking into multiple layers to search for domain-specialized sub-circuits\n",
    "vision_layers = [model.visual.transformer.resblocks[i] for i in range(model.visual.transformer.layers)]\n",
    "text_layers = [model.transformer.resblocks[i] for i in range(model.transformer.layers)]\n",
    "\n",
    "# We'll record activations in a dictionary for each domain condition\n",
    "domain_activations = {\"face_images\": [], \"non_face_images\": [], \"language_task\": []}\n",
    "\n",
    "# We'll define hook functions that store the mean activation for each layer,\n",
    "# grouped by domain (face_images, non_face_images, language_task).\n",
    "# ...\n",
    "\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    \"\"\"\n",
    "    module: the layer we hooked\n",
    "    input: a tuple of Tensors (the input to that layer)\n",
    "    output: the layer's output (Tensor)\n",
    "    \"\"\"\n",
    "    # output typically has shape [batch_size, seq_len, hidden_dim] for Transformers\n",
    "    # For simplicity, store the mean activation across seq_len\n",
    "    # But you might store all token activations for deeper analysis\n",
    "    activations['vision_block0_mlp'] = output.detach().cpu()\n",
    "\n",
    "# Register the forward hook\n",
    "hook_handle = target_layer.register_forward_hook(hook_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Identifying Highly Responsive Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_unit_responses = {}  # Will store mean activation for each concept\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, texts, concept_labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        # We'll encode text to force the text tower to run too (but let's focus on the vision tower example).\n",
    "        # If you want to investigate the text tower, register hooks similarly in model.transformer layers.\n",
    "\n",
    "        # forward pass\n",
    "        image_features = model.encode_image(images)\n",
    "        # text_features = model.encode_text(clip.tokenize(texts).to(device))  # if needed\n",
    "\n",
    "        # At this point, our hook \"hook_fn\" has run, so `activations['vision_block0_mlp']` is set\n",
    "        block_activs = activations['vision_block0_mlp'].cpu()  # shape: [B, seq_len, hidden_dim]\n",
    "\n",
    "        for i, concept in enumerate(concept_labels):\n",
    "            if concept not in concept_unit_responses:\n",
    "                concept_unit_responses[concept] = []\n",
    "            # Suppose we average across seq_len, then we have shape [hidden_dim]\n",
    "            # block_activs[i] is shape [seq_len, hidden_dim]\n",
    "            concept_mean = block_activs[i].mean(dim=0)  # shape [hidden_dim]\n",
    "            concept_unit_responses[concept].append(concept_mean.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to mean vectors across all samples for each concept\n",
    "for concept, activ_list in concept_unit_responses.items():\n",
    "    activ_tensor = torch.tensor(np.stack(activ_list))  # shape [N_samples, hidden_dim]\n",
    "    concept_unit_responses[concept] = activ_tensor.mean(dim=0)  # shape [hidden_dim]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Selecting “Highly Responsive” Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_units_to_select = 10  # pick top 10\n",
    "concept_top_units = {}\n",
    "\n",
    "for concept, mean_activ in concept_unit_responses.items():\n",
    "    # mean_activ is shape [hidden_dim]\n",
    "    # get top indices\n",
    "    values, indices = torch.topk(mean_activ, k=num_units_to_select)\n",
    "    concept_top_units[concept] = indices.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Ablation (Lesion) & Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AblationHook:\n",
    "    \"\"\"\n",
    "    A forward hook object that zeroes out certain hidden-dim units\n",
    "    in the MLP output.\n",
    "    \"\"\"\n",
    "    def __init__(self, units_to_ablate):\n",
    "        self.units_to_ablate = units_to_ablate  # List or set of indices\n",
    "\n",
    "    def __call__(self, module, input, output):\n",
    "        # output has shape [batch_size, seq_len, hidden_dim]\n",
    "        output[..., self.units_to_ablate] = 0\n",
    "        return output\n",
    "\n",
    "# Example usage:\n",
    "units_to_ablate = concept_top_units[\"dog\"]  # top dog units\n",
    "ablation_hook = AblationHook(units_to_ablate)\n",
    "\n",
    "# We attach the ablation hook in place of the normal forward hook:\n",
    "hook_handle.remove()  # remove the old \"recording\" hook\n",
    "ablation_handle = target_layer.register_forward_hook(ablation_hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get zero-shot classification accuracy\n",
    "def evaluate_zeroshot_accuracy(model, dataloader, classnames):\n",
    "    \"\"\"\n",
    "    classnames: list of possible classes, e.g. [\"dog\", \"cat\"]\n",
    "    We'll prompt-engineer them as \"a photo of a {classname}\".\n",
    "    \"\"\"\n",
    "    texts = [f\"a photo of a {cn}\" for cn in classnames]\n",
    "    text_tokens = clip.tokenize(texts).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_embs = model.encode_text(text_tokens)\n",
    "        text_embs = text_embs / text_embs.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = list(labels)  # strings\n",
    "\n",
    "            image_embs = model.encode_image(images)\n",
    "            image_embs = image_embs / image_embs.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # compute similarity\n",
    "            logits = 100.0 * image_embs @ text_embs.T  # shape [B, n_classes]\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            for i, label in enumerate(labels):\n",
    "                pred_class = classnames[preds[i]]\n",
    "                if pred_class == label:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "# Evaluate baseline (no ablation):\n",
    "baseline_acc = evaluate_zeroshot_accuracy(model, dataloader, classnames=[\"dog\", \"cat\"])\n",
    "\n",
    "# Evaluate after ablation:\n",
    "ablation_acc = evaluate_zeroshot_accuracy(model, dataloader, classnames=[\"dog\", \"cat\"])\n",
    "\n",
    "print(f\"Baseline accuracy: {baseline_acc:.4f}, After ablation: {ablation_acc:.4f}\")\n",
    "ablation_handle.remove()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Iterating Over Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "for concept in [\"dog\", \"cat\", \"face\", \"house\"]:\n",
    "    units_to_ablate = concept_top_units[concept]  # from previous analysis\n",
    "    ablation_hook = AblationHook(units_to_ablate)\n",
    "    ablation_handle = target_layer.register_forward_hook(ablation_hook)\n",
    "\n",
    "    # Evaluate\n",
    "    ablation_acc = evaluate_zeroshot_accuracy(model, dataloader, classnames=[\"dog\", \"cat\", \"face\", \"house\"])\n",
    "    ablation_handle.remove()\n",
    "\n",
    "    # Store\n",
    "    all_results.append({\n",
    "        \"concept\": concept,\n",
    "        \"ablated_units\": units_to_ablate,\n",
    "        \"zeroshot_acc\": ablation_acc\n",
    "    })\n",
    "\n",
    "# Inspect or write to file\n",
    "import json\n",
    "with open(\"ablation_results.json\", \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
