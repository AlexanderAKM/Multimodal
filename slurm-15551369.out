The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) 2023.01   2) StdEnv

The following have been reloaded with a version change:
  1) GCCcore/12.3.0 => GCCcore/13.2.0
  2) Python/3.11.3-GCCcore-12.3.0 => Python/3.11.5-GCCcore-13.2.0
  3) SQLite/3.42.0-GCCcore-12.3.0 => SQLite/3.43.1-GCCcore-13.2.0
  4) Tcl/8.6.13-GCCcore-12.3.0 => Tcl/8.6.13-GCCcore-13.2.0
  5) XZ/5.4.2-GCCcore-12.3.0 => XZ/5.4.4-GCCcore-13.2.0
  6) binutils/2.40-GCCcore-12.3.0 => binutils/2.40-GCCcore-13.2.0
  7) bzip2/1.0.8-GCCcore-12.3.0 => bzip2/1.0.8-GCCcore-13.2.0
  8) libffi/3.4.4-GCCcore-12.3.0 => libffi/3.4.4-GCCcore-13.2.0
  9) libreadline/8.2-GCCcore-12.3.0 => libreadline/8.2-GCCcore-13.2.0
 10) ncurses/6.4-GCCcore-12.3.0 => ncurses/6.4-GCCcore-13.2.0
 11) zlib/1.2.13-GCCcore-12.3.0 => zlib/1.2.13-GCCcore-13.2.0

using python3
LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
> Running with model llava-hf/llava-1.5-7b-hf
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:03,  1.69s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:03<00:01,  1.60s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.46s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.51s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home1/s5193400/Multimodal/llm-localization/generate_lesion.py", line 148, in <module>
    run_eval(model, tokenizer=tokenizer, device=device)
  File "/home1/s5193400/Multimodal/llm-localization/eval.py", line 72, in run_eval
    results = glue_metric.compute(predictions=predictions[0], references=references)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home1/s5193400/.local/lib/python3.11/site-packages/evaluate/module.py", line 455, in compute
    self.add_batch(**inputs)
  File "/home1/s5193400/.local/lib/python3.11/site-packages/evaluate/module.py", line 546, in add_batch
    raise ValueError(error_msg) from None
ValueError: Mismatch in the number of predictions (181) and references (1)
cache//llava-1.5-7b-hf_network=language_pooling=last-token_range=100-100_perc=0.1_nunits=None_pretrained=True.npy
Loaded language mask with 131 units, with shape (32, 4096)
Language mask applied

================================================================================
Example 2:
The new rights are nice enough  Everyone really likes the newest benefits  1
This site includes a list of all award winners and a searchable database of Government Executive articles. The Government Executive articles housed on the website are not able to be searched. 2
uh i don't know i i have mixed emotions about him uh sometimes i like him but at the same times i love to see somebody beat him I like him for the most part, but would still enjoy seeing someone beat him.
--------------------------------------------------------------------------------
The new rights are nice enough  Everyone really likes the newest benefits  1
This site includes a list of all award winners and a searchable database of Government Executive articles. The Government Executive articles housed on the website are not able to be searched. 2
uh i don't know i i have mixed emotions about him uh sometimes i like him but at the same times i love to see somebody beat him I like him for the most part, but would still enjoy seeing someone beat him. 3
1. The speaker is expressing a positive opinion about the new rights and benefits that have been granted to them.
2. The speaker is expressing a positive opinion about the awards and articles on the website.
3. The speaker has mixed emotions about the President, sometimes liking him and other times wanting to see him defeated.
================================================================================

Predictions: [tensor([    1,   450,   716, 10462,   526,  7575,  3307, 29871,  7569,   650,
         2289,  4188,   267,   278,   716,   342, 23633,   259, 29896,    13,
         4013,  3268,  7805,   263,  1051,   310,   599,  9862,   281, 16697,
          322,   263,  2740,   519,  2566,   310, 10354, 28841,  7456, 29889,
          450, 10354, 28841,  7456,  9261,   287,   373,   278,  4700,   526,
          451,  2221,   304,   367, 17371, 29889, 29871, 29906,    13, 16099,
          474,  1016, 29915, 29873,  1073,   474,   474,   505, 12849, 23023,
         1080,  1048,  1075,   318, 29882,  6041,   474,   763,  1075,   541,
          472,   278,  1021,  3064,   474,  5360,   304,  1074, 18462, 16646,
         1075,   306,   763,  1075,   363,   278,  1556,   760, 29892,   541,
          723,  1603, 13389,  8790,  4856, 16646,  1075, 29889, 29871, 29941,
           13, 29896, 29889,   450, 25657,   338,  4653,   292,   263,  6374,
         9426,  1048,   278,   716, 10462,   322, 23633,   393,   505,  1063,
        16896,   304,   963, 29889,    13, 29906, 29889,   450, 25657,   338,
         4653,   292,   263,  6374,  9426,  1048,   278, 24441,   322,  7456,
          373,   278,  4700, 29889,    13, 29941, 29889,   450, 25657,   756,
        12849, 23023,  1080,  1048,   278,  7178, 29892,  6041,  4188,   292,
         1075,   322,   916,  3064, 24507,   304,  1074,  1075, 16235, 29889,
            2], device='cuda:0')]
References: [1]
LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
> Running with model llava-hf/llava-1.5-7b-hf
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:03,  1.68s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:03<00:01,  1.58s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.47s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.51s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home1/s5193400/Multimodal/llm-localization/generate_lesion.py", line 148, in <module>
    run_eval(model, tokenizer=tokenizer, device=device)
  File "/home1/s5193400/Multimodal/llm-localization/eval.py", line 72, in run_eval
    results = glue_metric.compute(predictions=predictions[0], references=references)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home1/s5193400/.local/lib/python3.11/site-packages/evaluate/module.py", line 455, in compute
    self.add_batch(**inputs)
  File "/home1/s5193400/.local/lib/python3.11/site-packages/evaluate/module.py", line 546, in add_batch
    raise ValueError(error_msg) from None
ValueError: Mismatch in the number of predictions (120) and references (1)
cache//llava-1.5-7b-hf_network=language_pooling=last-token_range=100-100_perc=0.1_nunits=None_pretrained=True.npy
Loaded language mask with 131 units, with shape (32, 4096)
Language mask applied

================================================================================
Example 2:
The new rights are nice enough  Everyone really likes the newest benefits  1
This site includes a list of all award winners and a searchable database of Government Executive articles. The Government Executive articles housed on the website are not able to be searched. 2
uh i don't know i i have mixed emotions about him uh sometimes i like him but at the same times i love to see somebody beat him I like him for the most part, but would still enjoy seeing someone beat him.
--------------------------------------------------------------------------------
The new rights are nice enough  Everyone really likes the newest benefits  1
This site includes a list of all award winners and a searchable database of Government Executive articles. The Government Executive articles housed on the website are not able to be searched. 2
uh i don't know i i have mixed emotions about him uh sometimes i like him but at the same times i love to see somebody beat him I like him for the most part, but would still enjoy seeing someone beat him. 3
1. What is the meaning of life
================================================================================

Predictions: [tensor([    1,   450,   716, 10462,   526,  7575,  3307, 29871,  7569,   650,
         2289,  4188,   267,   278,   716,   342, 23633,   259, 29896,    13,
         4013,  3268,  7805,   263,  1051,   310,   599,  9862,   281, 16697,
          322,   263,  2740,   519,  2566,   310, 10354, 28841,  7456, 29889,
          450, 10354, 28841,  7456,  9261,   287,   373,   278,  4700,   526,
          451,  2221,   304,   367, 17371, 29889, 29871, 29906,    13, 16099,
          474,  1016, 29915, 29873,  1073,   474,   474,   505, 12849, 23023,
         1080,  1048,  1075,   318, 29882,  6041,   474,   763,  1075,   541,
          472,   278,  1021,  3064,   474,  5360,   304,  1074, 18462, 16646,
         1075,   306,   763,  1075,   363,   278,  1556,   760, 29892,   541,
          723,  1603, 13389,  8790,  4856, 16646,  1075, 29889, 29871, 29941,
           13, 29896, 29889,  1724,   338,   278,  6593,   310,  2834,     2],
       device='cuda:0')]
References: [1]
LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
> Running with model llava-hf/llava-1.5-7b-hf
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:03,  1.70s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:03<00:01,  1.59s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.47s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.52s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home1/s5193400/Multimodal/llm-localization/generate_lesion.py", line 148, in <module>
    run_eval(model, tokenizer=tokenizer, device=device)
  File "/home1/s5193400/Multimodal/llm-localization/eval.py", line 72, in run_eval
    results = glue_metric.compute(predictions=predictions[0], references=references)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home1/s5193400/.local/lib/python3.11/site-packages/evaluate/module.py", line 455, in compute
    self.add_batch(**inputs)
  File "/home1/s5193400/.local/lib/python3.11/site-packages/evaluate/module.py", line 546, in add_batch
    raise ValueError(error_msg) from None
ValueError: Mismatch in the number of predictions (111) and references (1)
cache//None
Language mask applied

================================================================================
Example 2:
The new rights are nice enough  Everyone really likes the newest benefits  1
This site includes a list of all award winners and a searchable database of Government Executive articles. The Government Executive articles housed on the website are not able to be searched. 2
uh i don't know i i have mixed emotions about him uh sometimes i like him but at the same times i love to see somebody beat him I like him for the most part, but would still enjoy seeing someone beat him.
--------------------------------------------------------------------------------
The new rights are nice enough  Everyone really likes the newest benefits  1
This site includes a list of all award winners and a searchable database of Government Executive articles. The Government Executive articles housed on the website are not able to be searched. 2
uh i don't know i i have mixed emotions about him uh sometimes i like him but at the same times i love to see somebody beat him I like him for the most part, but would still enjoy seeing someone beat him. 3
================================================================================

Predictions: [tensor([    1,   450,   716, 10462,   526,  7575,  3307, 29871,  7569,   650,
         2289,  4188,   267,   278,   716,   342, 23633,   259, 29896,    13,
         4013,  3268,  7805,   263,  1051,   310,   599,  9862,   281, 16697,
          322,   263,  2740,   519,  2566,   310, 10354, 28841,  7456, 29889,
          450, 10354, 28841,  7456,  9261,   287,   373,   278,  4700,   526,
          451,  2221,   304,   367, 17371, 29889, 29871, 29906,    13, 16099,
          474,  1016, 29915, 29873,  1073,   474,   474,   505, 12849, 23023,
         1080,  1048,  1075,   318, 29882,  6041,   474,   763,  1075,   541,
          472,   278,  1021,  3064,   474,  5360,   304,  1074, 18462, 16646,
         1075,   306,   763,  1075,   363,   278,  1556,   760, 29892,   541,
          723,  1603, 13389,  8790,  4856, 16646,  1075, 29889, 29871, 29941,
            2], device='cuda:0')]
References: [1]

###############################################################################
HÃ¡brÃ³k Cluster
Job 15551369 for user s5193400
Finished at: Thu Mar  6 14:00:15 CET 2025

Job details:
============

Job ID                         : 15551369
Name                           : generate_lesion
User                           : s5193400
Partition                      : gpushort
Nodes                          : v100v2gpu13
Number of Nodes                : 1
Cores                          : 8
Number of Tasks                : 1
State                          : FAILED  
Submit                         : 2025-03-06T13:58:53
Start                          : 2025-03-06T13:58:54
End                            : 2025-03-06T14:00:11
Reserved walltime              : 00:30:00
Used walltime                  : 00:01:17
Used CPU time                  : 00:00:45 (Efficiency:  7.29%)
% User (Computation)           : 82.39%
% System (I/O)                 : 17.61%
Total memory reserved          : 32G
Maximum memory used            : 874.66M
Requested GPUs                 : 1
Allocated GPUs                 : v100=1
Max GPU utilization            : 23%
Max GPU memory used            : 13.53G

Acknowledgements:
=================

Please see this page for information about acknowledging HÃ¡brÃ³k in your publications:

https://wiki.hpc.rug.nl/habrok/introduction/scientific_output

################################################################################
